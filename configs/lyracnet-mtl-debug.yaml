MODEL :
  NAME: LyraCNet-Inductive
  # Model
  ARCHITECTURE: lyracnet-mtl
  EMBEDDING_SIZE: 1024 
  DEPTH: 28 
  NUM_BLOCKS: 4
  WIDEN_FACTOR: 8
  FREQUENCY_BINS: 84
  SAMPLE_RATE: 22050
  DOWNSAMPLE_FACTOR: 15 # might try out 20?
  L2_NORMALIZE: False # L2 normalize the embeddings.
  SIMILARITY_SEARCH: MCSS # ['MCSS', 'NNS', 'MIPS'] If L2_NORMALIZE is True, use 'MIPS' for faster search.
  CHECKPOINT_DIR: logs/checkpoints/
TRAIN:
    # Train and Validation, and Augmentation directories
    FEATURES_DIR: /data/audio/cqt/
    TRAIN_CLIQUES: /data/discogs/Discogs-VI-YT-20240701-light.json.train.rich
    VALIDATION_CLIQUES: /data/discogs/Discogs-VI-YT-20240701-light.json.val.rich
    M_PER_CLASS: 4 # Number of versions per clique during a training iteration.
    SCALE: normalize # ['normalize', 'upscale']
    MAX_LENGTH: 8000 # Number of frames to consider as context, before downsampling.
    MIN_LENGTH: 4000
    BATCH_SIZE: 8 # Number of versions in batch.
    EPOCHS: 200 # Number of epochs to train.
    AUGMENTATION:
      F: 27
      NUM_FREQ_MASKS: 1
      NUM_TIME_MASKS: 1
      REPLACE_WITH_ZERO: false
      T: 100
    LOSS:
      PROTOTYPICAL:
        WEIGHT: 1.0
        N_SUPPORT: 2
      TRIPLET:
        WEIGHT: 1 # Weight of the loss.
        MARGIN: 0.2 # Triplet loss margin
        POSITIVE_MINING: hard # ['hard', 'random', 'easy']
        NEGATIVE_MINING: hard # ['hard', 'random', 'semi-hard']        SQUARED_DISTANCE: False # Use squared distance in the loss.
        SQUARED_DISTANCE: False # Use squared distance in the loss.
        NON_ZERO_MEAN: True # If true, the zero losses in the batch are filtered out before averaging.
      CENTER:
        WEIGHT: 0.001 # Weight of the loss.
        NUM_CLS: 80_017 # Number of classes.
        FEAT_DIM: 1024 # Embedding dimension
        SHARE_BN: True
      SOFTMAX:
        LABEL_SMOOTHING: 0.1
        OUTPUT_DIM: 80_017 # No. of classes of Discogs-Training set instead of 30k in CoverHunter paper.
    LOSS_INDUCTIVE:
      RELEASE_GENRES: # must be key like in Discogs dataset
        LOSS_NAME: MULTILABEL
        STRATEGY: multilabel # ["random", "first", "multilabel", "smooth"]
        LABEL_SMOOTHING: 0.1
        PROJECTION: mlp # ["linear", "affine", "mlp"]
        AFTER_BLOCK: 4 # after which encoder block to apply the inductive loss
        NUM_BLOCKS: 2 # hidden dimension 
        OUTPUT_DIM: 13 # No. of classes of Discogs-Training set instead of 30k in CoverHunter paper.  OPTIMIZER: Adam # ['Adam', 'AdamW']
      RELEASE_STYLES: # must be key like in Discogs dataset
        LOSS_NAME: MULTILABEL
        STRATEGY: multilabel # ["random", "first", "multilabel", "smooth"]
        LABEL_SMOOTHING: 0.1
        PROJECTION: mlp # ["linear", "affine", "mlp"]
        AFTER_BLOCK: 4 # after which encoder block to apply the inductive loss
        NUM_BLOCKS: 2 # hidden dimension 
        OUTPUT_DIM: 500 # No. of classes of Discogs-Training set instead of 30k in CoverHunter paper.  OPTIMIZER: Adam # ['Adam', 'AdamW']
      RELEASED:
        WEIGHT: 0.000001
        LOSS_NAME: MSE
        STRATEGY: fill_random # ["random", "first", "multilabel", "smooth"]
        PROJECTION: mlp # ["linear", "affine", "mlp"]
        AFTER_BLOCK: 4 # after which encoder block to apply the inductive loss
        NUM_BLOCKS: 2 # hidden dimension 
        OUTPUT_DIM: 1 #    OPTIMIZER: Adam # ['Adam', 'AdamW']
    WEIGHT_DECAY: 0.00001 
    AUTOMATIC_MIXED_PRECISION: True # Use mixed precision training.
    LR: # see https://github.com/Liu-Feng-deeplearning/CoverHunter/blob/main/egs/covers80/config/hparams.yaml
      SCHEDULE: EXPONENTIAL-MIN_LR # ['NONE', 'STEP', 'MULTISTEP', 'EXPONENTIAL' 'EXPONENTIAL-MIN_LR', 'COSINE, LIN-WARMUP-PCWS'].
      LR: 0.001 
      PARAMS:
        ETA_MIN: 0.0001 # from 
        GAMMA: 0.9975
    CUDA_DETERMINISTIC: False # Set to True for deterministic training.
    CUDA_BENCHMARK: False # Set to True for benchmarking.
    CLIQUE_USAGE_RATIO: 0.01 # Fraction of training data to use. Useful for debugging. 1.0 to use 100% of data.