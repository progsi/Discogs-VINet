MODEL :
  NAME: Discogs-MIREXNet
  ARCHITECTURE: cqtnet
  CONV_CHANNEL: 40
  EMBEDDING_SIZE: 512
  NORMALIZATION: bn # ['bn', 'in', 'ibn']
  POOLING: adaptive_max # ['adaptive_max', 'gem', 'softpool']
  PROJECTION: linear # ['none', 'linear', 'affine', 'mlp']
  L2_NORMALIZE: True # L2 normalize the embeddings.
  SIMILARITY_SEARCH: MIPS # ['MCSS', 'NNS', 'MIPS'] If L2_NORMALIZE is True, use 'MIPS' for faster search.
  DOWNSAMPLE_FACTOR: 20 # Downsampling factor.
  CHECKPOINT_DIR: logs/checkpoints/

TRAIN:
  # Train and Validation, and Augmentation directories
  FEATURES_DIR: /data/audio/cqt/
  TRAIN_CLIQUES: /data/discogs/Discogs-VI-YT-20240701-light.json.train
  VALIDATION_CLIQUES: /data/discogs/Discogs-VI-YT-20240701-light.json.val
  M_PER_CLASS: 4 # Number of versions per clique during a training iteration.
  SCALE: normalize # ['normalize', 'upscale']
  CONTEXT_LENGTH: 7600 # Number of frames to consider as context, before downsampling.
  BATCH_SIZE: 216 # Number of versions in batch.
  EPOCHS: 40 # Number of epochs to train.
  LOSS:
    TRIPLET:
      MARGIN: 0.3 # Triplet loss margin
      POSITIVE_MINING: random # ['hard', 'random', 'easy']
      NEGATIVE_MINING: hard # ['hard', 'random', 'semi-hard']
      SQUARED_DISTANCE: False # Use squared distance in the loss.
      NON_ZERO_MEAN: True # If true, the zero losses in the batch are filtered out before averaging.
  OPTIMIZER: Adam # ['Adam', 'AdamW']
  AUTOMATIC_MIXED_PRECISION: True # Use mixed precision training.
  LR:
    SCHEDULE: LIN-WARMUP-PCWS # ['NONE', 'STEP', 'MULTISTEP', 'EXPONENTIAL', 'COSINE, LIN-WARMUP-PCWS'].
    LR: 0.0001 # Starting learning rate in case of a schedule or constant in case of NONE.
    PARAMS:
      MAX_LR: 0.01
      WARMUP_STEPS: 5
      MILESTONES_LRS:
        - [ 8, 0.001]
        - [13, 0.0001]
        - [20, 0.00001]
        - [30, 0.000001]
        - [38, 0.0000001]
  CUDA_DETERMINISTIC: False # Set to True for deterministic training.
  CUDA_BENCHMARK: False # Set to True for benchmarking.
  CLIQUE_USAGE_RATIO: 1.0 # Fraction of training data to use. Useful for debugging. 1.0 to use 100% of data.